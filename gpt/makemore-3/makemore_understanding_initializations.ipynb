{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Important point 1\n",
        "# if your weights are not properly initialised for last layer (i.e., if they are high), then the logits can be such that\n",
        "# for given contexts, either you are very confidently right, or VERY CONFIDENTLY\n",
        "# WRONG. thus, this VERY CONFIDENTLY WRONG thing leads to an unacceptably higher\n",
        "# loss at initialization!\n",
        "\n",
        "# Sol: initialise weights of the last layer by very small numbers\n",
        "\n",
        "# Note that this will eliminate the initial easy gains from loss function: We are\n",
        "# doing only USEFUL work during training."
      ],
      "metadata": {
        "id": "q7zcbXw6d7PO"
      },
      "id": "q7zcbXw6d7PO",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't initialise all the weights by 0's. Maintain some entropy in the loss\n",
        "# function rather :). Otherwise things can go wrong at times.i.e., gradient may not propagate backward."
      ],
      "metadata": {
        "id": "vaaAKTwnfiMO"
      },
      "id": "vaaAKTwnfiMO",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT POINT 2\n",
        "# Notice the outputs of the activation of each neuron.\n",
        "# if most of the activated outputs of neurons lie in flat regions of activation function graph, then\n",
        "# it means no matter how you tweak input, output of neuron after applying activation will never change.\n",
        "# i.e., gradient propagated backards by the activation function layer will be 0.\n",
        "# Thus, as a result, the weights of previous layer's neuron will never get trained :(\n",
        "\n",
        "# To avoid this, make sure that W@x is normalised and not very large or very small."
      ],
      "metadata": {
        "id": "svZpbm6sqQYQ"
      },
      "id": "svZpbm6sqQYQ",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Important Point 3\n",
        "# BUT What exact number less than 1 to multiply the W matirx with?\n",
        "\n",
        "# Important Point 4\n",
        "# For every activation function, we have a gain term [in the term of point 3]."
      ],
      "metadata": {
        "id": "j4bqFrZbGH0c"
      },
      "id": "j4bqFrZbGH0c",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Important point 5\n",
        "# For very deep networks, we can't manually ensure that W@x + b for each layer are normalised before going into the\n",
        "# activation function. Here comes BATCH NORMALIZATION.\n",
        "\n",
        "# 1. Batch Normalizing.\n",
        "# 2. Notice that we don't want to compel the distribution of hidden activations\n",
        "# to stay normal ALWAYS. We want backprop to tell us how it actually should be.\n",
        "# We only want to compel the distribution of hidden activations to be normal DURING INITIALIZATION.\n",
        "# Thus we have concept of bngain and bnbias.\n",
        "# 3. BN layer also computes rolling means/rolling sd.\n",
        "    # BATCH NORMALIZATION LAYER OPERATES DIFFERENTLY DURING DEPLOYMENT: EXTRA STEP AFTER TRAINING\n",
        "    # OR\n",
        "    # ROLLING MEAN AND ROLLING SD - Maintain 2 buffers to implement this."
      ],
      "metadata": {
        "id": "AnlJArVAKl0f"
      },
      "id": "AnlJArVAKl0f",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USE OF BATCH NORMALIZATION:\n",
        "# 1. HAS REGULARIZING EFFECT\n",
        "# 2. STABILIZES TRAINING\n",
        "\n",
        "# ADDING BIAS TO W*X BEFORE BN LAYER ARE USELESS :)\n",
        "\n",
        "# If batch size is very small, a high momentum can not allow the rolling mean/rolling SD\n",
        "# to converge to actual mean/actual SD.\n",
        "\n",
        "# All the operations done in BN layer are differentiable, hence we can do backward pass on BN layer."
      ],
      "metadata": {
        "id": "LzzjYHyTK0sc"
      },
      "id": "LzzjYHyTK0sc",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Important Point 6\n",
        "# OBSERVING the effect of GAIN on forward activations and backward gradients of each\n",
        "# layer of deep neural network."
      ],
      "metadata": {
        "id": "CulnFGbKtNsM"
      },
      "id": "CulnFGbKtNsM",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Important point 7\n",
        "# With batch norm, we can set any gain for the weights, and NOT worry about distributions\n",
        "# of activations and gradients for each layer to be different, but we still would need\n",
        "# to set the right learning rate so that update to data ratio for all layers is\n",
        "# sufficiently high and above a threshold.\n",
        "\n",
        "# i.e., with batch normalization, although highe gain does not affect distributions of\n",
        "# activations and gradients computed at each layer, but it does effect HOW FAST\n",
        "# THE LAYERS OF NEURAL NETWORK DO LEARNING"
      ],
      "metadata": {
        "id": "2WYayvv74Uoe"
      },
      "id": "2WYayvv74Uoe",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We observed various plots that allow us obsevre the dynamic performance of the network."
      ],
      "metadata": {
        "id": "C87SZTuRlzd3"
      },
      "id": "C87SZTuRlzd3",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f0vO40sRmQhq"
      },
      "id": "f0vO40sRmQhq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}